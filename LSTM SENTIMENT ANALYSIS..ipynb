{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyTL/EILk3n5etpzsR/+Wj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"v-gnE0t1ct7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAxpW36Eclky"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n"]},{"cell_type":"code","source":["\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D , Bidirectional\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from sklearn.utils import resample\n","from sklearn.utils import shuffle\n","from sklearn.metrics import confusion_matrix,classification_report\n","import re"],"metadata":{"id":"CheiCZO4crWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model:\n","    def __init__(self, datafile = '/content/drive/MyDrive/TRUE FOUNDRY INTERNSHIP/airline_sentiment_analysis.csv'):\n","        self.data = pd.read_csv(datafile)\n","        self.max_fatures = 2000\n","        self.batch_size = 128\n","    def split(self, test_size):\n","        tokenizer = Tokenizer(num_words=self.max_fatures, split=' ')\n","        tokenizer.fit_on_texts(self.data['text'].values)\n","        X = tokenizer.texts_to_sequences(self.data['text'].values)\n","        X = pad_sequences(X)\n","\n","        y = pd.get_dummies(self.data['airline_sentiment']).values\n","        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n","\n","    def preprocessor(self):\n","        # Remove HTML markup\n","        self.data['text'] = data['text'].apply(lambda x: x.lower())\n","        # removing special chars\n","        self.data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n","        self.data['text'] = data['text'].str.replace('rt','')\n","    def create_model(self,embed_dim=128,lstm_out = 196):\n","        model = Sequential()\n","        model.add(Embedding(self.max_fatures, embed_dim,input_length = 32))\n","        model.add(SpatialDropout1D(0.4))\n","        model.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n","        model.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\n","        model.add(Dense(2,activation='softmax'))\n","        model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","        print(model.summary())\n","        return model\n","    def fit(self, model):\n","        model.fit(self.X_train, self.Y_train, epochs = 15, batch_size=self.batch_size, verbose = 1)\n","        return model\n","    def evaluate(self):\n","        score,acc = model.evaluate(self.X_test, self.Y_test, verbose = 2, batch_size = self.batch_size)\n","        print(score,acc)\n","    def Save_tokenizer(self):\n","      # saving\n","        with open('/content/drive/MyDrive/TRUE FOUNDRY INTERNSHIP/tokenizer.pickle', 'wb') as handle:\n","            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    def classification_report(self):\n","        predict_x=model.predict(self.X_test) \n","        classes_x=np.argmax(predict_x,axis=1)\n","        df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':classes_x})\n","        df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n","        print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n","        print(classification_report(df_test.true, df_test.pred))\n","    def predict(self, model1,text):\n","        twt = [text]\n","        #vectorizing the tweet by the pre-fitted tokenizer instance\n","        twt = tokenizer1.texts_to_sequences(twt)\n","        #padding the tweet to have exactly the same shape as `embedding_2` input\n","        twt = pad_sequences(twt, maxlen=32, dtype='int32', value=0)\n","        print(twt)\n","        sentiment = model1.predict(twt,batch_size=1,verbose = 2)[0]\n","        if(np.argmax(sentiment) == 0):\n","            print(\"negative\")\n","        elif (np.argmax(sentiment) == 1):\n","            print(\"positive\")"],"metadata":{"id":"POYdhjrScwdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","  modle= Model()\n","  modle.preprocessor()\n","  modle.split(0.20)\n","  modle.Save_tokenizer()\n","  model= modle.create_model()\n","  modle.fit(model)\n","  modle.evaluate()"],"metadata":{"id":"6bcxxk6Bc0Lu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["                      precision   recall  f1-score   support\n","\n","                0       0.94      0.95      0.94      1862\n","                1       0.78      0.73      0.76       447\n","\n","    accuracy                               0.91      2309\n","    macro avg          0.86      0.84      0.85      2309\n","    weighted avg       0.91      0.91      0.91      2309"],"metadata":{"id":"O2Cggc45JGdo"}},{"cell_type":"code","source":[],"metadata":{"id":"h2S6wzNRJHJV"},"execution_count":null,"outputs":[]}]}